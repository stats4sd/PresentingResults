# Precision and variability {#precision}

```{r,echo=FALSE,message=FALSE}
library(tidyverse)
library(knitr)

fs<-read.csv("fallowsurvey.csv")

fs$hhsize<-ifelse(fs$adults!="6+",as.numeric(as.character(fs$adults)),6)+sample(0:3,replace=TRUE,prob=c(4,3,2,1),size = 1479)
fs$hhsize[is.na(fs$hhsize)]<-1
data1<-data.frame(n=10:1000,sd=NA,se=NA)

for(i in 10:1000){
    data1$sd[i-9]=sd(fs$hhsize[1:i])
    data1$se[i-9]=data1$sd[i-9]/sqrt(i)
}

```

## SECTION UNDER CONSTRUCTION

## Introduction

When presenting results it can be tempting to focus solely on the main patterns and trends we can identify through our analysis. But we always have to bear in mind the levels of uncertainty we have in those trends, and the underlying variability that will exist around those trends. So it is important to consider how we could incorporate these ideas of precision and variability into the way we present our results to assist our audience by providing as much relevant context to the results as we can.

## Video

```{r,echo=FALSE,out.width="100%"}

```

## Precision vs Variability

When considering this topic we need to be clear on the difference between *precision* and *variability*. Although these concepts are related, they tell us two different things both of which are important:

Precision refers to the uncertainty in the *estimates* or statistics that we calculate from our data. As the amount of data we have increases the amount of precision we have increases. For example - if we were trying to estimate mean household size based on a sample size of 1, then the precision in that mean would be extremely low. But if we had a sample size of 10,000 households we could estimate the mean from then we would have a much more precise estimate.   

Variability refers to the underlying variability in the sample, and in the population. This is not impacted by increasing the sample size, other than that increasing the sample size will increase the precision in our estimates of variability!


## Precision

### Confidence intervals vs. standard errors

When displaying precision around estimates there are two commonly used measurements - confidence intervals and standard errors. Both of these are useful, and they are highly related to one another, but it is extremely important to label your presentation so it is clear exactly what measure of precision is being shown. Different software packages make different choices about exactly how to display!

The standard error can be considered as "the standard deviation *of the estimate*". This is a function of the standard deviation *in the data* and the sample size. As our sample size increases the standard error of the estimates decreases. The ratio of the estimate to the standard error is what helps us determine statistical significance in hypothesis testing, although it can be tricky to intepret standard errors as numbers on their own.

This is why using confidence intervals are general preferred - these provide the range of values where we think the "true" value of the estimate would lie. In general it is conventional to present results using a 95% confidence interval, this has the nice statistical property of usually being equal the mean +- two times the standard error. As we increase the level of confidence of our interval, to 99% or 99.9%, then width of the confidence interval will increase - to be more confident that we have captured the true value in our interval we need to allow for more possible options.

### In tables

### Around trends

When showing the confidence interval of a trend, visually this is better represented by showing a shaded region around the trend line, rather than through use of multiple error bars, which would be misleading or messy, or through use of seperate lines, which are harder to interpret and less visually intuitive.


### Overload of error bars?

When presenting plots with lots of error bars this can quickly start to look messy and hard to interpret. In cases with lots of treatments, e.g. a varietal screening trial with 100+ different treatments each with 4 replications, presenting a single measure of precision - a pooled estimate from a statistical model - may be beneficial as it will allow focus to be placed onto the results, whilst still maintaining a display of the precision.


## Variability

### Plots which can display variability

There are lots of different plots available which can show variability and compare variability across groups. Error bars should never be used as a way of showing variability. Not only do they not provide a strong visual summary of variability the conventional use of error bars is to represent precision, so it is extremely likely your plot will be mis-interpreted by some or all of your audience.

#### Boxplots/Violin Plots

#### Histograms

#### Cumulative Frequency Plots

#### Scatter plots

### In tables. Is standard deviation always the answer?
